{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMqLKiX3+UZcD6hiXxY7jO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manasdeshpande125/da6401_assignment2-partA/blob/main/DL_ASG2_Q2_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Impporting Libraries and Setting device for GPU**"
      ],
      "metadata": {
        "id": "j2b8cASBdRMQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR4lgGAwapvv"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import pytorch_lightning as pl\n",
        "import torchvision\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wandb Login\n",
        "import wandb\n",
        "!wandb login 41a2853ea088e37bd0d456e78102e82edb455afc"
      ],
      "metadata": {
        "id": "tlB4y16Ra3zU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Enable DataParallel if multiple GPUs\n",
        "def prepare_model_for_device(model):\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
        "        model = nn.DataParallel(model)\n",
        "    return model.to(device)"
      ],
      "metadata": {
        "id": "vqOwG6mxa7ZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Download**"
      ],
      "metadata": {
        "id": "Haj-JcPIdbGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/wandb_datasets/nature_12K.zip -O nature_12K.zip\n",
        "!unzip -q nature_12K.zip\n",
        "!rm nature_12K.zip"
      ],
      "metadata": {
        "id": "oYiPzyW9a9y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Data Function: Here I have split /train into training and validation**"
      ],
      "metadata": {
        "id": "JX9x4FAOdjNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(batch_count, data_aug='n', train_dir='inaturalist_12K/train'):\n",
        "    if data_aug.lower() == 'y':\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.RandomCrop((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(degrees=(0, 30)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "    else:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "    dataset = ImageFolder(root=train_dir, transform=transform)\n",
        "    val_size = round(0.2 * len(dataset))\n",
        "    train_size = len(dataset) - val_size\n",
        "    train_ds, val_ds = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(10))\n",
        "\n",
        "    trainloader = DataLoader(train_ds, batch_size=batch_count, shuffle=True, num_workers=2)\n",
        "    validationloader = DataLoader(val_ds, batch_size=batch_count, shuffle=False, num_workers=2)\n",
        "\n",
        "    classes = dataset.classes\n",
        "    return trainloader, validationloader, classes\n"
      ],
      "metadata": {
        "id": "8qwAPDEpa_UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "trainloader, valloader, classes = load_data(batch_count=32, data_aug='y')\n",
        "print(\"Number of classes:\", len(classes))\n",
        "print(\"Classes:\", classes)"
      ],
      "metadata": {
        "id": "ofdHJwM6bDY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building a CNN with 5 layers using Pytorch_Lightning Module**"
      ],
      "metadata": {
        "id": "KWqGtPyAdwCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import WandbLogger  #Used for logging into wandb\n",
        "\n",
        "\n",
        "class LightningNet(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 input_shape=(3, 224, 224),\n",
        "                 filters=[4, 16, 32, 64, 128],\n",
        "                 kernel_size=[3, 3, 3, 3, 3],\n",
        "                 activation=nn.ReLU,\n",
        "                 batch_size=32,\n",
        "                 use_batch_norm=True,\n",
        "                 use_dropout=True,\n",
        "                 dropout_rate=0.25,\n",
        "                 learning_rate=1e-3,\n",
        "                 num_classes=10):   #default parameters\n",
        "\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.activation = activation()\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.use_dropout = use_dropout\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        in_channels = input_shape[0]\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "        self.bn_layers = nn.ModuleList()\n",
        "\n",
        "        assert len(filters) == len(kernel_size), \"filters and kernel_sizes must be the same length\"\n",
        "\n",
        "        for out_channels, k_size in zip(filters, kernel_size):\n",
        "            self.conv_layers.append(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=k_size, padding=k_size // 2)\n",
        "            )\n",
        "            if use_batch_norm:\n",
        "                self.bn_layers.append(nn.BatchNorm2d(out_channels))\n",
        "            in_channels = out_channels\n",
        "\n",
        "        # Compute flattened size after conv stack\n",
        "        self.flattened_size = self._get_conv_output(input_shape, batch_size)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(self.flattened_size, 84)\n",
        "        self.fc2 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def _get_conv_output(self, shape, batch_size):\n",
        "        dummy_input = torch.zeros(batch_size, *shape)\n",
        "        dummy_output = self._forward_features(dummy_input)\n",
        "        return dummy_output.view(batch_size, -1).size(1)\n",
        "\n",
        "    def _forward_features(self, x):\n",
        "        for i, conv in enumerate(self.conv_layers):\n",
        "            x = conv(x)\n",
        "            if self.use_batch_norm:\n",
        "                x = self.bn_layers[i](x)\n",
        "            x = self.activation(x)\n",
        "            x = self.pool(x)\n",
        "            if self.use_dropout:\n",
        "                x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._forward_features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        if self.use_dropout:\n",
        "            x = self.dropout(x)\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
        "        self.log('train_loss', loss, prog_bar=True, on_epoch=True, on_step=False)\n",
        "        self.log('train_acc', acc, prog_bar=True, on_epoch=True, on_step=False)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
        "        self.log('val_loss', loss, prog_bar=True, on_epoch=True, on_step=False)\n",
        "        self.log('val_acc', acc, prog_bar=True, on_epoch=True, on_step=False)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = (preds == y).float().mean()\n",
        "\n",
        "        self.log('test_loss', loss, prog_bar=True,on_epoch=True, on_step=False)\n",
        "        self.log('test_acc', acc, prog_bar=True,on_epoch=True, on_step=False)\n",
        "\n",
        "        return {'test_loss': loss, 'test_acc': acc}\n",
        "\n",
        "    # Commented out as it was showing some error in newer pytorch_lightning versions\n",
        "    # def test_epoch_end(self, outputs):\n",
        "    #     avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
        "    #     avg_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n",
        "    #     self.log('avg_test_loss', avg_loss,on_epoch=True, on_step=False)\n",
        "    #     self.log('avg_test_acc', avg_acc,on_epoch=True, on_step=False)\n",
        "\n",
        "    def configure_optimizers(self, optimizer_type='adam'):\n",
        "        if optimizer_type == 'sgd':\n",
        "            optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        elif optimizer_type == 'momentum':\n",
        "            optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate, momentum=0.9)\n",
        "\n",
        "        elif optimizer_type == 'nesterov':\n",
        "            optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate, momentum=0.9, nesterov=True)\n",
        "\n",
        "        elif optimizer_type == 'adam':\n",
        "            optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        elif optimizer_type == 'nadam':\n",
        "            optimizer = torch.optim.NAdam(self.parameters(), lr=self.learning_rate)\n"
      ],
      "metadata": {
        "id": "cUVF3rYObF_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn as nn\n",
        "\n",
        "# def val_eval(net, device, val_loader):\n",
        "#     net.eval()\n",
        "#     val_loss = 0.0\n",
        "#     val_total = 0\n",
        "#     val_correct = 0\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for inputs, labels in val_loader:\n",
        "#             inputs, labels = inputs.to(device), labels.to(device)\n",
        "#             outputs = net(inputs)\n",
        "#             loss = criterion(outputs, labels)\n",
        "\n",
        "#             val_loss += loss.item() * inputs.size(0)\n",
        "#             _, preds = torch.max(outputs, 1)\n",
        "#             val_total += labels.size(0)\n",
        "#             val_correct += (preds == labels).sum().item()\n",
        "\n",
        "#     avg_loss = val_loss / val_total\n",
        "#     accuracy = round((val_correct / val_total) * 100, 2)\n",
        "#     print(f\"Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "#     return accuracy, avg_loss"
      ],
      "metadata": {
        "id": "7jNsNEATd7ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.optim as optim\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# def train_CNN(epochs, filter_mode, act_str, batch_count, data_aug,\n",
        "#               batch_norm, drop, optimizer_name, lr_rate, drop_value):\n",
        "\n",
        "#     # Load dataset\n",
        "#     train_data, val_data, classes = load_data(batch_count, data_aug)\n",
        "\n",
        "#     # Filter configurations\n",
        "#     filter_map = {\n",
        "#         'all_32':   [32, 32, 32, 32, 32],\n",
        "#         'inc':      [16, 32, 64, 128, 256],\n",
        "#         'dec':      [128, 64, 32, 16, 8],\n",
        "#         'inc_dec':  [32, 64, 128, 64, 32],\n",
        "#         'dec_inc':  [128, 64, 32, 64, 128],\n",
        "#     }\n",
        "\n",
        "#     filters = filter_map.get(filter_mode, [32, 64, 128, 64, 32])  # default fallback\n",
        "\n",
        "#     # Activation function\n",
        "#     activation_map = {\n",
        "#         'relu': nn.ReLU(),\n",
        "#         'sigmoid': nn.Sigmoid(),\n",
        "#         'tanh': nn.Tanh(),\n",
        "#         'gelu': nn.GELU(),\n",
        "#         'silu': nn.SiLU()\n",
        "#     }\n",
        "#     act_fn = activation_map.get(act_str.lower(), nn.ReLU())  # default to ReLU\n",
        "\n",
        "\n",
        "#     net = LightningNet(\n",
        "#     input_shape=(3, 224, 224),\n",
        "#     filters=[16, 32, 64, 128, 256],\n",
        "#     kernel_size=3,\n",
        "#     activation=nn.GELU,\n",
        "#     use_batch_norm=True,\n",
        "#     use_dropout=True,\n",
        "#     dropout_rate=0.3,\n",
        "#     learning_rate=1e-4,\n",
        "#     num_classes=len(classes)  # from earlier data loader\n",
        "#     )\n",
        "\n",
        "#     trainer = pl.Trainer(max_epochs=10, accelerator='auto', devices=1)\n",
        "\n",
        "\n",
        "#     # Loss function\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#     # Optimizer\n",
        "#     optimizer_map = {\n",
        "#         'SGD': optim.SGD(net.parameters(), lr=lr_rate, momentum=0.9),\n",
        "#         'Adam': optim.Adam(net.parameters(), lr=lr_rate),\n",
        "#         'RMSProp': optim.RMSprop(net.parameters(), lr=lr_rate, momentum=0.9)\n",
        "#     }\n",
        "#     optimizer = optimizer_map.get(optimizer_name, optim.Adam(net.parameters(), lr=lr_rate))\n",
        "\n",
        "#     for epoch in range(1, epochs + 1):\n",
        "#         # net.train()\n",
        "#         trainer.fit(net, trainloader, valloader)\n",
        "#         train_loss = 0.0\n",
        "#         train_total = 0\n",
        "#         train_correct = 0\n",
        "\n",
        "#         for inputs, labels in tqdm(train_data, desc=f\"Epoch {epoch}/{epochs}\"):\n",
        "#             inputs, labels = inputs.to(device), labels.to(device)\n",
        "#             optimizer.zero_grad()\n",
        "\n",
        "#             outputs = net(inputs)\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             train_loss += loss.item() * inputs.size(0)\n",
        "#             _, preds = torch.max(outputs, 1)\n",
        "#             train_total += labels.size(0)\n",
        "#             train_correct += (preds == labels).sum().item()\n",
        "\n",
        "#         train_avg_loss = train_loss / train_total\n",
        "#         train_acc = round((train_correct / train_total) * 100, 2)\n",
        "\n",
        "#         print(f\"Epoch {epoch}: Train Loss = {train_avg_loss:.4f}, Accuracy = {train_acc:.2f}%\", end=\" | \")\n",
        "\n",
        "#         val_acc, val_loss = val_eval(net, device, val_data)\n",
        "\n",
        "#         # Example wandb logging\n",
        "#         wandb.log({\n",
        "#             \"epoch\": epoch,\n",
        "#             \"train_loss\": train_avg_loss,\n",
        "#             \"train_acc\": train_acc,\n",
        "#             \"val_loss\": val_loss,\n",
        "#             \"val_acc\": val_acc\n",
        "#         })\n",
        "\n",
        "#     return train_avg_loss, train_acc, val_acc"
      ],
      "metadata": {
        "id": "s5cTisRLdzve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_CNN(\n",
        "#     epochs=5,\n",
        "#     filter_mode='inc_dec',\n",
        "#     act_str='relu',\n",
        "#     batch_count=32,\n",
        "#     data_aug='y',\n",
        "#     batch_norm='y',\n",
        "#     drop='y',\n",
        "#     optimizer_name='Adam',\n",
        "#     lr_rate=1e-3,\n",
        "#     drop_value=0.3\n",
        "# )"
      ],
      "metadata": {
        "id": "bKLmrWnKeDa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to calculate loss and Accuracy**"
      ],
      "metadata": {
        "id": "y6snZMRFexdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def val_eval_lightning(model, val_loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    acc = round((total_correct / total_samples) * 100, 2)\n",
        "    return acc, avg_loss\n"
      ],
      "metadata": {
        "id": "zBRTjFafedhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function which creates model for LightningNet class and trains it**"
      ],
      "metadata": {
        "id": "83qsjzHHe1kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_CNN_lightning_manual(epochs, filter_mode, act_str, batch_count, data_aug,\n",
        "                                batch_norm, drop, optimizer_name, lr_rate, drop_value):\n",
        "\n",
        "    # 1. Load dataset\n",
        "    train_loader, val_loader, classes = load_data(batch_count, data_aug)\n",
        "\n",
        "    # 2. Filter configurations\n",
        "    filter_map = {\n",
        "        'all_32':   [32, 32, 32, 32, 32],\n",
        "        'inc':      [16, 32, 64, 128, 256],\n",
        "        'dec':      [128, 64, 32, 16, 8],\n",
        "        'inc_dec':  [32, 64, 128, 64, 32],\n",
        "        'dec_inc':  [128, 64, 32, 64, 128],\n",
        "    }\n",
        "    filters = filter_map.get(filter_mode, [32, 64, 128, 64, 32])\n",
        "\n",
        "    # 3. Activation function\n",
        "    activation_map = {\n",
        "        'relu': nn.ReLU,\n",
        "        'sigmoid': nn.Sigmoid,\n",
        "        'tanh': nn.Tanh,\n",
        "        'gelu': nn.GELU,\n",
        "        'silu': nn.SiLU\n",
        "    }\n",
        "    activation_fn = activation_map.get(act_str.lower(), nn.ReLU)\n",
        "\n",
        "    # 4. Instantiate model\n",
        "    model = LightningNet(\n",
        "        input_shape=(3, 224, 224),\n",
        "        filters=filters,\n",
        "        kernel_size=3,\n",
        "        activation=activation_fn,\n",
        "        batch_size=batch_count,\n",
        "        use_batch_norm=(batch_norm == 'y'),\n",
        "        use_dropout=(drop == 'y'),\n",
        "        dropout_rate=drop_value,\n",
        "        learning_rate=lr_rate,\n",
        "        num_classes=len(classes)\n",
        "    ).to(device)\n",
        "\n",
        "    # 5. Define optimizer\n",
        "    optimizer_map = {\n",
        "        'sgd': optim.SGD(model.parameters(), lr=lr_rate, momentum=0.9),\n",
        "        'adam': optim.Adam(model.parameters(), lr=lr_rate),\n",
        "        'rmsprop': optim.RMSprop(model.parameters(), lr=lr_rate, momentum=0.9)\n",
        "    }\n",
        "    optimizer = optimizer_map.get(optimizer_name.lower(), optim.Adam(model.parameters(), lr=lr_rate))\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 6. Training loop\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)  #class forward function\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = total_loss / total\n",
        "        train_acc = round((correct / total) * 100, 2)\n",
        "\n",
        "        # 7. Validation after each epoch\n",
        "        val_acc, val_loss = val_eval_lightning(model, val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch}: \"\n",
        "              f\"Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.2f}%, \"\n",
        "              f\"Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.2f}%\\n\")\n",
        "\n",
        "    return model  # return model if you want to save/checkpoint later\n"
      ],
      "metadata": {
        "id": "CltNNujReSW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = train_CNN_lightning_manual(\n",
        "    epochs=2,\n",
        "    filter_mode='inc',\n",
        "    act_str='relu',\n",
        "    batch_count=32,\n",
        "    data_aug='y',\n",
        "    batch_norm='y',\n",
        "    drop='y',\n",
        "    optimizer_name='adam',\n",
        "    lr_rate=1e-3,\n",
        "    drop_value=0.25\n",
        ")"
      ],
      "metadata": {
        "id": "N6bWb6Bseg52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Specifying the Sweep Configuration**"
      ],
      "metadata": {
        "id": "pFkur4ZnfEFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_configuration = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "        'name': 'val_Accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {'values': [10,20]},\n",
        "        'hidden_layer_size': {'values': [84, 96, 128]},\n",
        "        'learning_rate': {'values': [1e-3, 1e-4]},\n",
        "        'weight_decay': {'values': [0, 0.0005, 0.5]},\n",
        "        'optimizer_name': {\n",
        "            'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']\n",
        "        },\n",
        "        'batch_size': {'values': [16, 32, 64]},\n",
        "        'activation_type': {'values': ['sigmoid', 'tanh', 'ReLU', 'GELU', 'SiLU', 'Mish']},\n",
        "        'loss_type': {'values': ['cross_entropy']},\n",
        "        'filters': {\n",
        "            'values': [\n",
        "                'all_32',\n",
        "                'inc',\n",
        "                'dec',\n",
        "                'inc_dec',\n",
        "                'dec_inc',\n",
        "\n",
        "            ]\n",
        "        },\n",
        "        'data_augmentation': {'values': [True, False]},\n",
        "        'use_batch_norm': {'values': [True, False]},\n",
        "        'use_dropout': {'values': [True, False]},\n",
        "        'dropout_rate': {'values': [0.2, 0.3]},\n",
        "\n",
        "        # NEW kernel size combinations\n",
        "        'kernel_sizes': {\n",
        "            'values': [\n",
        "                [3, 3, 3, 3, 3],\n",
        "                [3, 3, 5, 3, 3],\n",
        "                [5, 3, 5, 3, 5],\n",
        "                [3, 5, 3, 5, 3],\n",
        "                [5 ,5 , 3, 3, 3]\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "JyDVVQShbKdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to call train and test function based on config parameters with early stopping**"
      ],
      "metadata": {
        "id": "fu8QJPlZfJVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "def train_CNN_lightning(epochs, filter_mode, act_str, batch_count, data_aug,\n",
        "                        batch_norm, drop, optimizer_name, lr_rate, drop_value,kernel_sizes,test_loader=None):\n",
        "\n",
        "    # 1. Load dataset\n",
        "    train_loader, val_loader, classes = load_data(batch_count, data_aug)\n",
        "    #test_loader, test_classes = load_test_data(batch_count)\n",
        "\n",
        "    # 2. Define filter configurations\n",
        "    filter_map = {\n",
        "        'all_32':   [32, 32, 32, 32, 32],\n",
        "        'inc':      [16, 32, 64, 128, 256],\n",
        "        'dec':      [128, 64, 32, 16, 8],\n",
        "        'inc_dec':  [32, 64, 128, 64, 32],\n",
        "        'dec_inc':  [128, 64, 32, 64, 128],\n",
        "    }\n",
        "    filters = filter_map.get(filter_mode, [32, 64, 128, 64, 32])\n",
        "    if len(kernel_sizes) != len(filters):\n",
        "        raise ValueError(\"Length of kernel_sizes must match number of filters.\")\n",
        "\n",
        "    # 3. Activation functions\n",
        "    activation_map = {\n",
        "        'relu': nn.ReLU,\n",
        "        'sigmoid': nn.Sigmoid,\n",
        "        'tanh': nn.Tanh,\n",
        "        'gelu': nn.GELU,\n",
        "        'silu': nn.SiLU,\n",
        "        'mish': nn.Mish\n",
        "    }\n",
        "    activation_fn = activation_map.get(act_str.lower(), nn.ReLU)\n",
        "\n",
        "    # 4. Optimizer mapping passed as string to LightningNet\n",
        "    optimizer_name = optimizer_name.lower()  # For consistency\n",
        "\n",
        "    # 5. Model initialization\n",
        "    model = LightningNet(\n",
        "        input_shape=(3, 224, 224),\n",
        "        filters=filters,\n",
        "        kernel_size=kernel_sizes,\n",
        "        activation=activation_fn,\n",
        "        batch_size=batch_count,\n",
        "        use_batch_norm=(batch_norm == 'y'),\n",
        "        use_dropout=(drop == 'y'),\n",
        "        dropout_rate=drop_value,\n",
        "        learning_rate=lr_rate,\n",
        "        num_classes=len(classes)\n",
        "    )\n",
        "\n",
        "    # 6. WandB logger (Optional, comment if not using wandb)\n",
        "    wandb_logger = WandbLogger(project=\"DA6401-assignment-2\", log_model=True)\n",
        "\n",
        "    # 7. Callbacks (optional: Early stopping, Checkpointing)\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=3, mode='min'),\n",
        "        ModelCheckpoint(monitor='val_loss', mode='min', save_top_k=1, filename='{epoch}-{val_loss:.2f}')\n",
        "    ]\n",
        "\n",
        "    # 8. Trainer setup\n",
        "    trainer = Trainer(\n",
        "        max_epochs=epochs,\n",
        "        accelerator='auto',\n",
        "        devices=1 if torch.cuda.is_available() else None,\n",
        "        callbacks=callbacks,\n",
        "        logger=wandb_logger\n",
        "    )\n",
        "\n",
        "    # 9. Train the model\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "    # 10. Test the model\n",
        "    trainer.test(model, dataloaders=test_loader)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "SUp-hDqqbN8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sweep Function**"
      ],
      "metadata": {
        "id": "JSynAKbefULS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "def sweep_train():\n",
        "    wandb.init(project=\"DA6401-Assignment-2\", entity=\"cs24m024-iit-madras-org\", config=sweep_configuration)\n",
        "    config = wandb.config\n",
        "\n",
        "    # Set up run name for easier tracking in W&B\n",
        "    wandb.run.name = f\"e_{config.epochs}_lr_{config.learning_rate}_wd_{config.weight_decay}_o_{config.optimizer_name}_bs_{config.batch_size}_ac_{config.activation_type}_los_{config.loss_type}\"\n",
        "\n",
        "\n",
        "    # Call your training function\n",
        "    train_CNN_lightning(\n",
        "        epochs=config.epochs,\n",
        "        filter_mode=config.filters,\n",
        "        act_str=config.activation_type,\n",
        "        batch_count=config.batch_size,\n",
        "        data_aug='y' if config.data_augmentation else 'n',\n",
        "        batch_norm='y' if config.use_batch_norm else 'n',\n",
        "        drop='y' if config.use_dropout else 'n',\n",
        "        optimizer_name=config.optimizer_name,\n",
        "        lr_rate=config.learning_rate,\n",
        "        drop_value=config.dropout_rate,\n",
        "        kernel_sizes=config.kernel_sizes\n",
        "    )\n"
      ],
      "metadata": {
        "id": "qiSG7SBdbZNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_configuration,project='DA6401-Assignment-2')\n",
        "wandb.agent(sweep_id,function=sweep_train,project='DA6401-Assignment-2',count=100)"
      ],
      "metadata": {
        "id": "A19j0eQGbfTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To calculate top 3 sweep configurations for validation accuracy**"
      ],
      "metadata": {
        "id": "Dz_8pEVsfXV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entity = 'cs24m024-iit-madras'\n",
        "project = 'DA6401-assignment-2'\n",
        "sweep_ids = [\n",
        "    '37haj8j1',   # Sweep 1 ID\n",
        "    'u47hemx2',   # Sweep 2 ID\n",
        "]\n",
        "\n",
        "# Connect to the API\n",
        "api = wandb.Api()\n",
        "\n",
        "# Gather runs from both sweeps\n",
        "all_runs = []\n",
        "for sweep_id in sweep_ids:\n",
        "    sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
        "    runs = sweep.runs\n",
        "    for run in runs:\n",
        "        if run.state == 'finished' and 'val_acc' in run.summary:\n",
        "            all_runs.append(run)\n",
        "\n",
        "# Sort runs based on val_acc\n",
        "top_runs = sorted(all_runs, key=lambda r: r.summary['val_acc'], reverse=True)[:3]\n",
        "\n",
        "# Print top 3 models across both sweeps\n",
        "print(\"Top 3 runs by validation accuracy (across both sweeps):\")\n",
        "for i, run in enumerate(top_runs, 1):\n",
        "    print(f\"\\nModel #{i}\")\n",
        "    print(f\"Name        : {run.name}\")\n",
        "    print(f\"Run ID      : {run.id}\")\n",
        "    print(f\"Sweep ID    : {run.sweep.id}\")\n",
        "    print(f\"Val Accuracy: {run.summary['val_acc']:.4f}\")\n",
        "    print(f\"Config      : {run.config}\")"
      ],
      "metadata": {
        "id": "AiHAXrF8eqZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing on top 3 accuracies of model and showing predictions in grid of images**"
      ],
      "metadata": {
        "id": "TjqjwQxLffUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"DA6401-Assignment-2\", name=\"testing\")"
      ],
      "metadata": {
        "id": "cauI3LV8fFJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pytorch_lightning import Trainer\n",
        "\n",
        "# 1. Load test dataset\n",
        "def load_test_data(test_dir='inaturalist_12K/val', batch_size=32):\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    test_dataset = ImageFolder(root=test_dir, transform=test_transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    return test_loader, test_dataset.classes\n",
        "\n",
        "# 2. Visualization\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    return np.transpose(npimg, (1, 2, 0))\n",
        "\n",
        "def show_predictions(model, dataloader, classes, rows=10, cols=3):\n",
        "    model.eval()\n",
        "    images_shown = 0\n",
        "    plt.figure(figsize=(15, 30))\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(model.device)\n",
        "            labels = labels.to(model.device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            for i in range(images.size(0)):\n",
        "                if images_shown >= rows * cols:\n",
        "                    break\n",
        "                plt.subplot(rows, cols, images_shown + 1)\n",
        "                plt.imshow(imshow(images[i].cpu()))\n",
        "                plt.title(f\"Pred: {classes[preds[i]]}\\nTrue: {classes[labels[i]]}\", fontsize=8)\n",
        "                plt.axis('off')\n",
        "                images_shown += 1\n",
        "            if images_shown >= rows * cols:\n",
        "                break\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 3. Load test data once\n",
        "test_loader, test_classes = load_test_data()\n",
        "\n",
        "# 4. Train and evaluate all 3 models\n",
        "\n",
        "# Model 1 – Top Val Accuracy: 0.4095\n",
        "model1 = train_CNN_lightning(\n",
        "    epochs=20,\n",
        "    filter_mode=\"inc_dec\",\n",
        "    act_str=\"RELU\",\n",
        "    batch_count=64,\n",
        "    data_aug='y',\n",
        "    batch_norm='y',\n",
        "    drop='n',\n",
        "    optimizer_name=\"adam\",\n",
        "    lr_rate=0.001,\n",
        "    drop_value=0.2,\n",
        "    kernel_sizes=[3, 3, 3, 3, 3],\n",
        "    test_loader=test_loader\n",
        ")\n",
        "# trainer1 = Trainer(accelerator='auto', devices=1 if torch.cuda.is_available() else None)\n",
        "# trainer1.test(model1, test_loader)\n",
        "show_predictions(model1, test_loader, test_classes)\n",
        "\n",
        "# Model 2 – Val Accuracy: 0.4005\n",
        "model2 = train_CNN_lightning(\n",
        "    epochs=20,\n",
        "    filter_mode=\"dec_inc\",\n",
        "    act_str=\"Mish\",\n",
        "    batch_count=64,\n",
        "    data_aug='y',\n",
        "    batch_norm='y',\n",
        "    drop='n',\n",
        "    optimizer_name=\"momentum\",\n",
        "    lr_rate=0.0001,\n",
        "    drop_value=0.3,\n",
        "    kernel_sizes=[5, 5, 3, 3, 3],\n",
        "    test_loader=test_loader\n",
        ")\n",
        "# trainer2 = Trainer(accelerator='auto', devices=1 if torch.cuda.is_available() else None)\n",
        "# trainer2.test(model2, test_loader)\n",
        "show_predictions(model2, test_loader, test_classes)\n",
        "\n",
        "# Model 3 – Val Accuracy: 0.3960\n",
        "model3 = train_CNN_lightning(\n",
        "    epochs=13,\n",
        "    filter_mode=\"dec_inc\",\n",
        "    act_str=\"GELU\",\n",
        "    batch_count=32,\n",
        "    data_aug='y',\n",
        "    batch_norm='y',\n",
        "    drop='n',\n",
        "    optimizer_name=\"nadam\",\n",
        "    lr_rate=0.0001,\n",
        "    drop_value=0.2,\n",
        "    kernel_sizes=[3, 3, 3, 3, 3],\n",
        "    test_loader=test_loader\n",
        ")\n",
        "# trainer3 = Trainer(accelerator='auto', devices=1 if torch.cuda.is_available() else None)\n",
        "# trainer3.test(model3, test_loader)\n",
        "show_predictions(model3, test_loader, test_classes)\n"
      ],
      "metadata": {
        "id": "tAbk3cTHes_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting Confusion Matrix**"
      ],
      "metadata": {
        "id": "ICxWWjJNfkPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"DA6401-Assignment-2\", name=\"confusion_matrix1\")"
      ],
      "metadata": {
        "id": "2ZGB0khLml6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "model1.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, labels in test_loader:\n",
        "        data = data.to(device)\n",
        "        outputs = model1(data)\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "# Log the confusion matrix to wandb\n",
        "wandb.log({\n",
        "    \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
        "        probs=None,\n",
        "        y_true=all_labels,\n",
        "        preds=all_preds,\n",
        "        class_names=test_classes\n",
        "    )\n",
        "})\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues', xticklabels=test_classes, yticklabels=test_classes)\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BGE0kXEcmFLB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}